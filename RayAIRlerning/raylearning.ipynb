{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 16:39:10,396\tINFO worker.py:1807 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CPU': 24.0,\n",
      " 'GPU': 1.0,\n",
      " 'accelerator_type:G': 1.0,\n",
      " 'memory': 3689597339.0,\n",
      " 'node:172.20.221.186': 1.0,\n",
      " 'node:__internal_head__': 1.0,\n",
      " 'object_store_memory': 1844798668.0}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import ray\n",
    "ray.init()\n",
    "pprint(ray.cluster_resources())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 16:47:20,557\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import ray\n",
    "from ray.data import Dataset, Preprocessor\n",
    "from ray.data.preprocessors import StandardScaler\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.train import Result, ScalingConfig\n",
    "import xgboost\n",
    "use_gpu = False\n",
    "num_workers = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data() -> Tuple[Dataset, Dataset, Dataset]:\n",
    "    dataset = ray.data.read_csv(\"s3://anonymous@air-example-data/breast_cancer.csv\")\n",
    "    train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)\n",
    "    test_dataset = valid_dataset.drop_columns([\"target\"])\n",
    "    return train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(num_workers: int, use_gpu: bool = False) -> Result:\n",
    "    train_dataset, valid_dataset, _ = prepare_data()\n",
    "\n",
    "    # Scale some random columns\n",
    "    columns_to_scale = [\"mean radius\", \"mean texture\"]\n",
    "    preprocessor = StandardScaler(columns=columns_to_scale)\n",
    "    train_dataset = preprocessor.fit_transform(train_dataset)\n",
    "    valid_dataset = preprocessor.transform(valid_dataset)\n",
    "\n",
    "    # XGBoost specific params\n",
    "    params = {\n",
    "        \"tree_method\": \"approx\",\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "    }\n",
    "\n",
    "    trainer = XGBoostTrainer(\n",
    "        scaling_config=ScalingConfig(num_workers=num_workers, use_gpu=use_gpu),\n",
    "        label_column=\"target\",\n",
    "        params=params,\n",
    "        datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    "        num_boost_round=100,\n",
    "        metadata = {\"preprocessor_pkl\": preprocessor.serialize()}\n",
    "    )\n",
    "    result = trainer.fit()\n",
    "    print(result.metrics)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ray.train import Checkpoint\n",
    "\n",
    "\n",
    "class Predict:\n",
    "\n",
    "    def __init__(self, checkpoint: Checkpoint):\n",
    "        self.model = XGBoostTrainer.get_model(checkpoint)\n",
    "        self.preprocessor = Preprocessor.deserialize(checkpoint.get_metadata()[\"preprocessor_pkl\"])\n",
    "\n",
    "    def __call__(self, batch: pd.DataFrame) -> pd.DataFrame:\n",
    "        preprocessed_batch = self.preprocessor.transform_batch(batch)\n",
    "        dmatrix = xgboost.DMatrix(preprocessed_batch)\n",
    "        return {\"predictions\": self.model.predict(dmatrix)}\n",
    "\n",
    "\n",
    "def predict_xgboost(result: Result):\n",
    "    _, _, test_dataset = prepare_data()\n",
    "\n",
    "    scores = test_dataset.map_batches(\n",
    "        Predict, \n",
    "        fn_constructor_args=[result.checkpoint], \n",
    "        concurrency=1, \n",
    "        batch_format=\"pandas\"\n",
    "    )\n",
    "    \n",
    "    predicted_labels = scores.map_batches(lambda df: (df > 0.5).astype(int), batch_format=\"pandas\")\n",
    "    print(f\"PREDICTED LABELS\")\n",
    "    predicted_labels.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 16:48:19,637\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-10-30_16-39-09_402576_721550/logs/ray-data\n",
      "2024-10-30 16:48:19,638\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadCSV]\n",
      "                                                                                                                  \n",
      "✔️  Dataset execution finished in 11.46 seconds: 100%|██████████| 569/569 [00:11<00:00, 49.6 row/s]                          \n",
      "\n",
      "- ReadCSV->SplitBlocks(48): Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 27.4KB object store: : 569 row [00:11, 49.6 row/s]\n",
      "2024-10-30 16:48:31,120\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-10-30_16-39-09_402576_721550/logs/ray-data\n",
      "2024-10-30 16:48:31,121\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadCSV]\n",
      "                                                                                                                  \n",
      "✔️  Dataset execution finished in 5.38 seconds: 100%|██████████| 569/569 [00:05<00:00, 106 row/s]                            \n",
      "\n",
      "- ReadCSV->SplitBlocks(48): Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 16.0KB object store: : 569 row [00:05, 106 row/s]\n",
      "2024-10-30 16:48:36,536\tINFO dataset.py:2529 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2024-10-30 16:48:36,538\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-10-30_16-39-09_402576_721550/logs/ray-data\n",
      "2024-10-30 16:48:36,539\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=1]\n",
      "Running 0: 0.00 row [00:00, ? row/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                                    \n",
      "\u001b[A                                     \n",
      "\n",
      "\u001b[A\u001b[A                                                      \n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A                                                   \n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A                                                   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "✔️  Dataset execution finished in 0.67 seconds: 100%|██████████| 1.00/1.00 [00:00<00:00, 1.48 row/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A                                                                                                                                                \n",
      "\n",
      "\u001b[A\u001b[A                                                      \n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A                                                   \n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A                                                   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "- Aggregate: Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 0.0B object store; 1 rows output: 100%|██████████| 1.00/1.00 [00:00<00:00, 1.45 row/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A                                                   \n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A                                                   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  *- Sort Sample: : 0.00 row [00:00, ? row/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A                                                          \n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A                                                   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  *- Shuffle Map: 100%|██████████| 398/398 [00:00<00:00, 561 row/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A                                                             \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  *- Shuffle Reduce: 100%|██████████| 1.00/1.00 [00:00<00:00, 1.40 row/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "- limit=1: Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 32.0B object store: 100%|██████████| 1.00/1.00 [00:00<00:00, 1.37 row/s]\n",
      "2024-10-30 16:48:37,295\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2024-10-30 16:48:38,600\tINFO data_parallel_trainer.py:340 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-10-30 16:48:38 (running for 00:00:00.11)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 11.0/24 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2024-10-30_16-39-09_402576_721550/artifacts/2024-10-30_16-48-37/XGBoostTrainer_2024-10-30_16-48-37/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m - (node_id=d3facf9112a40477b403d0144a025de36aee0cf1253cfb4842b96cb1, ip=172.20.221.186, pid=728460) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m - (node_id=d3facf9112a40477b403d0144a025de36aee0cf1253cfb4842b96cb1, ip=172.20.221.186, pid=728461) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m - (node_id=d3facf9112a40477b403d0144a025de36aee0cf1253cfb4842b96cb1, ip=172.20.221.186, pid=728459) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m - (node_id=d3facf9112a40477b403d0144a025de36aee0cf1253cfb4842b96cb1, ip=172.20.221.186, pid=728462) world_rank=3, local_rank=3, node_rank=0\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m - (node_id=d3facf9112a40477b403d0144a025de36aee0cf1253cfb4842b96cb1, ip=172.20.221.186, pid=728458) world_rank=4, local_rank=4, node_rank=0\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m - (node_id=d3facf9112a40477b403d0144a025de36aee0cf1253cfb4842b96cb1, ip=172.20.221.186, pid=728464) world_rank=5, local_rank=5, node_rank=0\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m - (node_id=d3facf9112a40477b403d0144a025de36aee0cf1253cfb4842b96cb1, ip=172.20.221.186, pid=728465) world_rank=6, local_rank=6, node_rank=0\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m - (node_id=d3facf9112a40477b403d0144a025de36aee0cf1253cfb4842b96cb1, ip=172.20.221.186, pid=728466) world_rank=7, local_rank=7, node_rank=0\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m - (node_id=d3facf9112a40477b403d0144a025de36aee0cf1253cfb4842b96cb1, ip=172.20.221.186, pid=728467) world_rank=8, local_rank=8, node_rank=0\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m - (node_id=d3facf9112a40477b403d0144a025de36aee0cf1253cfb4842b96cb1, ip=172.20.221.186, pid=728463) world_rank=9, local_rank=9, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=728464)\u001b[0m [16:48:41] Task [xgboost.ray-rank=00000005]:7b523a257e70ad14cc4c3bd301000000 got rank 5\n",
      "(pid=729069) Running 0: 0.00 row [00:00, ? row/s]\n",
      "                                                 \n",
      "\u001b[A                                                       \n",
      "\n",
      "\u001b[A\u001b[A                                                           \u001b[36m(SplitCoordinator pid=729069)\u001b[0m Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-10-30_16-39-09_402576_721550/logs/ray-data\n",
      "(pid=729069) Running 0: 0.00 row [00:00, ? row/s]\n",
      "                                                 \n",
      "\u001b[A                                                       \n",
      "\n",
      "\u001b[A\u001b[A                                                           \u001b[36m(SplitCoordinator pid=729069)\u001b[0m Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[StandardScaler] -> OutputSplitter[split(10, equal=True)]\n",
      "(pid=729069) Running 0: 0.00 row [00:00, ? row/s]\n",
      "(pid=729069) Running Dataset. Active & requested resources: 0/13 CPU, 40.0KB/879.7MB object store:  59%|█████▉    | 240/408 [00:01<00:00, 234 row/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                                                                                                                    \n",
      "\u001b[A                                                                                                                                                  \n",
      "\n",
      "(pid=729069) ✔️  Dataset execution finished in 1.10 seconds: 100%|██████████| 371/371 [00:01<00:00, 352 row/s]                                                                   \n",
      "\n",
      "\u001b[A\n",
      "\u001b[A                                                                                                                                                 \n",
      "\n",
      "(pid=729069) - StandardScaler: Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 2.0KB object store: 100%|██████████| 398/398 [00:01<00:00, 373 row/s]                             \n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "(pid=729069) - split(10, equal=True): Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 38.0KB object store; [locality disabled]: 100%|██████████| 390/390 [00:01<00:00, 363 row/s]\n",
      "(pid=729068) Running 0: 0.00 row [00:00, ? row/s]\n",
      "(pid=729068) Running Dataset. Active & requested resources: 1/13 CPU, 43.4KB/879.7MB object store: : 0.00 row [00:01, ? row/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                                                                                                                     \n",
      "\u001b[A                                                                                                                                                   \n",
      "\n",
      "\u001b[A\u001b[A                                                                                                                                                                          \u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [0]\ttrain-logloss:0.46569\ttrain-error:0.05897\tvalid-logloss:0.46750\tvalid-error:0.08824\n",
      "(pid=729068) Running Dataset. Active & requested resources: 1/13 CPU, 14.0KB/879.7MB object store:  68%|██████▊   | 116/171 [00:02<00:00, 56.9 row/s]\n",
      "                                                                                                                                                     \n",
      "\u001b[A                                                                                                                                                   \n",
      "\n",
      "(pid=729068) ✔️  Dataset execution finished in 2.10 seconds: 100%|██████████| 132/132 [00:02<00:00, 61.6 row/s]                                                                 \n",
      "\n",
      "\u001b[A\n",
      "\u001b[A                                                                                                                                                   \n",
      "\n",
      "(pid=729068) - StandardScaler: Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 260.0B object store: 100%|██████████| 171/171 [00:02<00:00, 79.5 row/s]                          \n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "(pid=729068) - split(10, equal=True): Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 13.7KB object store; [locality disabled]: 100%|██████████| 170/170 [00:02<00:00, 78.6 row/s]\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [1]\ttrain-logloss:0.33095\ttrain-error:0.01795\tvalid-logloss:0.36897\tvalid-error:0.07647\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [2]\ttrain-logloss:0.24961\ttrain-error:0.01026\tvalid-logloss:0.30369\tvalid-error:0.07059\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [3]\ttrain-logloss:0.19162\ttrain-error:0.00769\tvalid-logloss:0.25403\tvalid-error:0.05882\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [4]\ttrain-logloss:0.14804\ttrain-error:0.00769\tvalid-logloss:0.21797\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [5]\ttrain-logloss:0.11754\ttrain-error:0.00256\tvalid-logloss:0.19655\tvalid-error:0.04706\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [6]\ttrain-logloss:0.09435\ttrain-error:0.00256\tvalid-logloss:0.17407\tvalid-error:0.04706\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [7]\ttrain-logloss:0.07744\ttrain-error:0.00256\tvalid-logloss:0.16415\tvalid-error:0.05294\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [8]\ttrain-logloss:0.06484\ttrain-error:0.00256\tvalid-logloss:0.15255\tvalid-error:0.05294\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [9]\ttrain-logloss:0.05410\ttrain-error:0.00256\tvalid-logloss:0.14110\tvalid-error:0.04706\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [10]\ttrain-logloss:0.04584\ttrain-error:0.00000\tvalid-logloss:0.13014\tvalid-error:0.04706\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [11]\ttrain-logloss:0.04042\ttrain-error:0.00000\tvalid-logloss:0.12102\tvalid-error:0.05882\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [12]\ttrain-logloss:0.03505\ttrain-error:0.00000\tvalid-logloss:0.11301\tvalid-error:0.04706\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [13]\ttrain-logloss:0.03105\ttrain-error:0.00000\tvalid-logloss:0.10892\tvalid-error:0.05294\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [14]\ttrain-logloss:0.02798\ttrain-error:0.00000\tvalid-logloss:0.10853\tvalid-error:0.05294\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [15]\ttrain-logloss:0.02559\ttrain-error:0.00000\tvalid-logloss:0.10806\tvalid-error:0.05294\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [16]\ttrain-logloss:0.02379\ttrain-error:0.00000\tvalid-logloss:0.10902\tvalid-error:0.05294\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [17]\ttrain-logloss:0.02164\ttrain-error:0.00000\tvalid-logloss:0.10640\tvalid-error:0.05294\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [18]\ttrain-logloss:0.01964\ttrain-error:0.00000\tvalid-logloss:0.10352\tvalid-error:0.05294\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [19]\ttrain-logloss:0.01828\ttrain-error:0.00000\tvalid-logloss:0.10064\tvalid-error:0.05294\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [20]\ttrain-logloss:0.01721\ttrain-error:0.00000\tvalid-logloss:0.10025\tvalid-error:0.04706\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [21]\ttrain-logloss:0.01635\ttrain-error:0.00000\tvalid-logloss:0.10055\tvalid-error:0.04706\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [22]\ttrain-logloss:0.01541\ttrain-error:0.00000\tvalid-logloss:0.09980\tvalid-error:0.04706\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [23]\ttrain-logloss:0.01467\ttrain-error:0.00000\tvalid-logloss:0.09936\tvalid-error:0.04706\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [24]\ttrain-logloss:0.01421\ttrain-error:0.00000\tvalid-logloss:0.09724\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [25]\ttrain-logloss:0.01372\ttrain-error:0.00000\tvalid-logloss:0.09729\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [26]\ttrain-logloss:0.01297\ttrain-error:0.00000\tvalid-logloss:0.09527\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [27]\ttrain-logloss:0.01253\ttrain-error:0.00000\tvalid-logloss:0.09454\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [28]\ttrain-logloss:0.01217\ttrain-error:0.00000\tvalid-logloss:0.09473\tvalid-error:0.04706\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [29]\ttrain-logloss:0.01182\ttrain-error:0.00000\tvalid-logloss:0.09326\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [30]\ttrain-logloss:0.01142\ttrain-error:0.00000\tvalid-logloss:0.09092\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [31]\ttrain-logloss:0.01109\ttrain-error:0.00000\tvalid-logloss:0.09139\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:45] [32]\ttrain-logloss:0.01074\ttrain-error:0.00000\tvalid-logloss:0.09033\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [33]\ttrain-logloss:0.01045\ttrain-error:0.00000\tvalid-logloss:0.08830\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [34]\ttrain-logloss:0.01021\ttrain-error:0.00000\tvalid-logloss:0.08688\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [35]\ttrain-logloss:0.00994\ttrain-error:0.00000\tvalid-logloss:0.08733\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [36]\ttrain-logloss:0.00974\ttrain-error:0.00000\tvalid-logloss:0.08750\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [37]\ttrain-logloss:0.00953\ttrain-error:0.00000\tvalid-logloss:0.08620\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [38]\ttrain-logloss:0.00927\ttrain-error:0.00000\tvalid-logloss:0.08443\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [39]\ttrain-logloss:0.00913\ttrain-error:0.00000\tvalid-logloss:0.08375\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [40]\ttrain-logloss:0.00903\ttrain-error:0.00000\tvalid-logloss:0.08363\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [41]\ttrain-logloss:0.00883\ttrain-error:0.00000\tvalid-logloss:0.08311\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [42]\ttrain-logloss:0.00866\ttrain-error:0.00000\tvalid-logloss:0.08163\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [43]\ttrain-logloss:0.00857\ttrain-error:0.00000\tvalid-logloss:0.08204\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [44]\ttrain-logloss:0.00849\ttrain-error:0.00000\tvalid-logloss:0.08247\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [45]\ttrain-logloss:0.00831\ttrain-error:0.00000\tvalid-logloss:0.08283\tvalid-error:0.04118\n",
      "\u001b[36m(RayTrainWorker pid=728465)\u001b[0m [16:48:41] Task [xgboost.ray-rank=00000006]:48df60aae250ba295611aa0901000000 got rank 6\u001b[32m [repeated 9x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [46]\ttrain-logloss:0.00824\ttrain-error:0.00000\tvalid-logloss:0.08296\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [47]\ttrain-logloss:0.00816\ttrain-error:0.00000\tvalid-logloss:0.08328\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [48]\ttrain-logloss:0.00809\ttrain-error:0.00000\tvalid-logloss:0.08384\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [49]\ttrain-logloss:0.00801\ttrain-error:0.00000\tvalid-logloss:0.08376\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [50]\ttrain-logloss:0.00795\ttrain-error:0.00000\tvalid-logloss:0.08285\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [51]\ttrain-logloss:0.00788\ttrain-error:0.00000\tvalid-logloss:0.08258\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [52]\ttrain-logloss:0.00782\ttrain-error:0.00000\tvalid-logloss:0.08266\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [53]\ttrain-logloss:0.00776\ttrain-error:0.00000\tvalid-logloss:0.08322\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [54]\ttrain-logloss:0.00770\ttrain-error:0.00000\tvalid-logloss:0.08343\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [55]\ttrain-logloss:0.00764\ttrain-error:0.00000\tvalid-logloss:0.08323\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [56]\ttrain-logloss:0.00758\ttrain-error:0.00000\tvalid-logloss:0.08331\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [57]\ttrain-logloss:0.00752\ttrain-error:0.00000\tvalid-logloss:0.08253\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [58]\ttrain-logloss:0.00747\ttrain-error:0.00000\tvalid-logloss:0.08291\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [59]\ttrain-logloss:0.00742\ttrain-error:0.00000\tvalid-logloss:0.08268\tvalid-error:0.04118\n",
      "\u001b[36m(SplitCoordinator pid=729068)\u001b[0m Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-10-30_16-39-09_402576_721550/logs/ray-data\n",
      "\u001b[36m(SplitCoordinator pid=729068)\u001b[0m Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[StandardScaler] -> OutputSplitter[split(10, equal=True)]\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [60]\ttrain-logloss:0.00736\ttrain-error:0.00000\tvalid-logloss:0.08269\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [61]\ttrain-logloss:0.00731\ttrain-error:0.00000\tvalid-logloss:0.08250\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [62]\ttrain-logloss:0.00726\ttrain-error:0.00000\tvalid-logloss:0.08266\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [63]\ttrain-logloss:0.00721\ttrain-error:0.00000\tvalid-logloss:0.08201\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [64]\ttrain-logloss:0.00716\ttrain-error:0.00000\tvalid-logloss:0.08211\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [65]\ttrain-logloss:0.00711\ttrain-error:0.00000\tvalid-logloss:0.08232\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [66]\ttrain-logloss:0.00706\ttrain-error:0.00000\tvalid-logloss:0.08213\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [67]\ttrain-logloss:0.00702\ttrain-error:0.00000\tvalid-logloss:0.08214\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [68]\ttrain-logloss:0.00697\ttrain-error:0.00000\tvalid-logloss:0.08155\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [69]\ttrain-logloss:0.00692\ttrain-error:0.00000\tvalid-logloss:0.08083\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [70]\ttrain-logloss:0.00688\ttrain-error:0.00000\tvalid-logloss:0.08026\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [71]\ttrain-logloss:0.00684\ttrain-error:0.00000\tvalid-logloss:0.08040\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [72]\ttrain-logloss:0.00679\ttrain-error:0.00000\tvalid-logloss:0.07984\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [73]\ttrain-logloss:0.00676\ttrain-error:0.00000\tvalid-logloss:0.07985\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [74]\ttrain-logloss:0.00672\ttrain-error:0.00000\tvalid-logloss:0.07988\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [75]\ttrain-logloss:0.00668\ttrain-error:0.00000\tvalid-logloss:0.08040\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [76]\ttrain-logloss:0.00664\ttrain-error:0.00000\tvalid-logloss:0.07972\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [77]\ttrain-logloss:0.00661\ttrain-error:0.00000\tvalid-logloss:0.07958\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:46] [78]\ttrain-logloss:0.00657\ttrain-error:0.00000\tvalid-logloss:0.07901\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:47] [79]\ttrain-logloss:0.00653\ttrain-error:0.00000\tvalid-logloss:0.07912\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:47] [80]\ttrain-logloss:0.00649\ttrain-error:0.00000\tvalid-logloss:0.07893\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:47] [81]\ttrain-logloss:0.00646\ttrain-error:0.00000\tvalid-logloss:0.07842\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:47] [82]\ttrain-logloss:0.00643\ttrain-error:0.00000\tvalid-logloss:0.07846\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:47] [83]\ttrain-logloss:0.00639\ttrain-error:0.00000\tvalid-logloss:0.07797\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:47] [84]\ttrain-logloss:0.00636\ttrain-error:0.00000\tvalid-logloss:0.07811\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:47] [85]\ttrain-logloss:0.00633\ttrain-error:0.00000\tvalid-logloss:0.07790\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:47] [86]\ttrain-logloss:0.00630\ttrain-error:0.00000\tvalid-logloss:0.07775\tvalid-error:0.04706\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:47] [87]\ttrain-logloss:0.00627\ttrain-error:0.00000\tvalid-logloss:0.07788\tvalid-error:0.04706\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:47] [88]\ttrain-logloss:0.00624\ttrain-error:0.00000\tvalid-logloss:0.07802\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:47] [89]\ttrain-logloss:0.00621\ttrain-error:0.00000\tvalid-logloss:0.07799\tvalid-error:0.04706\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:47] [90]\ttrain-logloss:0.00619\ttrain-error:0.00000\tvalid-logloss:0.07759\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:47] [91]\ttrain-logloss:0.00615\ttrain-error:0.00000\tvalid-logloss:0.07738\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:47] [92]\ttrain-logloss:0.00613\ttrain-error:0.00000\tvalid-logloss:0.07694\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:47] [93]\ttrain-logloss:0.00610\ttrain-error:0.00000\tvalid-logloss:0.07707\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:47] [94]\ttrain-logloss:0.00608\ttrain-error:0.00000\tvalid-logloss:0.07665\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:47] [95]\ttrain-logloss:0.00605\ttrain-error:0.00000\tvalid-logloss:0.07677\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:47] [96]\ttrain-logloss:0.00603\ttrain-error:0.00000\tvalid-logloss:0.07684\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:47] [97]\ttrain-logloss:0.00600\ttrain-error:0.00000\tvalid-logloss:0.07643\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:47] [98]\ttrain-logloss:0.00598\ttrain-error:0.00000\tvalid-logloss:0.07654\tvalid-error:0.04118\n",
      "\u001b[36m(XGBoostTrainer pid=728375)\u001b[0m [16:48:47] [99]\ttrain-logloss:0.00596\ttrain-error:0.00000\tvalid-logloss:0.07639\tvalid-error:0.04118\n",
      "\u001b[36m(RayTrainWorker pid=728460)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/sat/ray_results/XGBoostTrainer_2024-10-30_16-48-37/XGBoostTrainer_22b6a_00000_0_2024-10-30_16-48-38/checkpoint_000000)\n",
      "\u001b[33m(raylet)\u001b[0m [2024-10-30 16:49:10,396 E 721893 721893] (raylet) node_manager.cc:3069: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: d3facf9112a40477b403d0144a025de36aee0cf1253cfb4842b96cb1, IP: 172.20.221.186) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.20.221.186`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-10-31 14:53:11,180 E 721893 721893] (raylet) node_manager.cc:3069: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: d3facf9112a40477b403d0144a025de36aee0cf1253cfb4842b96cb1, IP: 172.20.221.186) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.20.221.186`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2024-10-31 22:53:11,459 E 721893 721893] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: d3facf9112a40477b403d0144a025de36aee0cf1253cfb4842b96cb1, IP: 172.20.221.186) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.20.221.186`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-11-01 00:19:11,509 E 721893 721893] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: d3facf9112a40477b403d0144a025de36aee0cf1253cfb4842b96cb1, IP: 172.20.221.186) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.20.221.186`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2024-11-01 01:16:11,543 E 721893 721893] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: d3facf9112a40477b403d0144a025de36aee0cf1253cfb4842b96cb1, IP: 172.20.221.186) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.20.221.186`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-11-01 02:37:11,590 E 721893 721893] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: d3facf9112a40477b403d0144a025de36aee0cf1253cfb4842b96cb1, IP: 172.20.221.186) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.20.221.186`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2024-11-01 03:56:11,635 E 721893 721893] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: d3facf9112a40477b403d0144a025de36aee0cf1253cfb4842b96cb1, IP: 172.20.221.186) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.20.221.186`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-11-01 05:00:11,672 E 721893 721893] (raylet) node_manager.cc:3069: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: d3facf9112a40477b403d0144a025de36aee0cf1253cfb4842b96cb1, IP: 172.20.221.186) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.20.221.186`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2024-11-01 08:36:11,797 E 721893 721893] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: d3facf9112a40477b403d0144a025de36aee0cf1253cfb4842b96cb1, IP: 172.20.221.186) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.20.221.186`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-11-01 09:45:11,838 E 721893 721893] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: d3facf9112a40477b403d0144a025de36aee0cf1253cfb4842b96cb1, IP: 172.20.221.186) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.20.221.186`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "\u001b[A                                                       \n",
      "\n",
      "(pid=729068) Running 0: 0.00 row [00:00, ? row/s]                \n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-10-30 16:48:43 (running for 00:00:05.15)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 11.0/24 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2024-10-30_16-39-09_402576_721550/artifacts/2024-10-30_16-48-37/XGBoostTrainer_2024-10-30_16-48-37/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 16:48:48,501\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/sat/ray_results/XGBoostTrainer_2024-10-30_16-48-37' in 0.0069s.\n",
      "2024-10-30 16:48:48,505\tINFO tune.py:1041 -- Total run time: 11.21 seconds (9.90 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-10-30 16:48:48 (running for 00:00:09.90)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 11.0/24 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2024-10-30_16-39-09_402576_721550/artifacts/2024-10-30_16-48-37/XGBoostTrainer_2024-10-30_16-48-37/driver_artifacts\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "\n",
      "\n",
      "OrderedDict([('train-logloss', np.float64(0.00595748214697009)), ('train-error', np.float64(0.0)), ('valid-logloss', np.float64(0.07639243121041493)), ('valid-error', np.float64(0.04117647058823528)), ('timestamp', 1730281727), ('checkpoint_dir_name', 'checkpoint_000000'), ('should_checkpoint', True), ('done', True), ('training_iteration', 101), ('trial_id', '22b6a_00000'), ('date', '2024-10-30_16-48-47'), ('time_this_iter_s', 0.005759477615356445), ('time_total_s', 7.536529064178467), ('pid', 728375), ('hostname', 'DESKTOP-VIHKCAB'), ('node_ip', '172.20.221.186'), ('config', {}), ('time_since_restore', 7.536529064178467), ('iterations_since_restore', 101), ('experiment_tag', '0')])\n"
     ]
    }
   ],
   "source": [
    "result = train_xgboost(num_workers=10, use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 16:50:27,276\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-10-30_16-39-09_402576_721550/logs/ray-data\n",
      "2024-10-30 16:50:27,277\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadCSV]\n",
      "                                                                                                                  \n",
      "✔️  Dataset execution finished in 7.00 seconds: 100%|██████████| 569/569 [00:06<00:00, 81.3 row/s]                           \n",
      "\n",
      "- ReadCSV->SplitBlocks(48): Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 27.4KB object store: : 569 row [00:07, 81.3 row/s]\n",
      "2024-10-30 16:50:34,286\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-10-30_16-39-09_402576_721550/logs/ray-data\n",
      "2024-10-30 16:50:34,287\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadCSV]\n",
      "                                                                                                                  \n",
      "✔️  Dataset execution finished in 7.19 seconds: 100%|██████████| 569/569 [00:07<00:00, 79.2 row/s]                           \n",
      "\n",
      "- ReadCSV->SplitBlocks(48): Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 5.3KB object store: : 569 row [00:07, 79.1 row/s]\n",
      "2024-10-30 16:50:41,505\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-10-30_16-39-09_402576_721550/logs/ray-data\n",
      "2024-10-30 16:50:41,505\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(drop_columns)->MapBatches(Predict)] -> TaskPoolMapOperator[MapBatches(<lambda>)] -> LimitOperator[limit=20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED LABELS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running 0: 0.00 row [00:00, ? row/s]\n",
      "\u001b[A\n",
      "\n",
      "                                                                                                               \n",
      "\u001b[A                                                                         \n",
      "\n",
      "\u001b[A\u001b[A                                             \n",
      "\n",
      "\n",
      "✔️  Dataset execution finished in 1.35 seconds: 100%|██████████| 20.0/20.0 [00:01<00:00, 14.8 row/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A                                                                                                                                                                                               \n",
      "\n",
      "\u001b[A\u001b[A                                             \n",
      "\n",
      "\n",
      "- MapBatches(drop_columns)->MapBatches(Predict): Tasks: 3; Actors: 1; Queued blocks: 0; Resources: 1.0 CPU, 222.0B object store; [locality off]: 100%|██████████| 138/138 [00:00<00:00, 931 row/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                                                                                                                         \n",
      "\n",
      "\n",
      "- MapBatches(<lambda>): Tasks: 4; Queued blocks: 0; Resources: 4.0 CPU, 1.7KB object store: 100%|██████████| 94.0/94.0 [00:00<00:00, 591 row/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "- limit=20: Tasks: 0; Queued blocks: 4; Resources: 0.0 CPU, 208.0B object store: 100%|██████████| 20.0/20.0 [00:00<00:00, 121 row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': np.int64(1)}\n",
      "{'predictions': np.int64(1)}\n",
      "{'predictions': np.int64(0)}\n",
      "{'predictions': np.int64(1)}\n",
      "{'predictions': np.int64(1)}\n",
      "{'predictions': np.int64(1)}\n",
      "{'predictions': np.int64(1)}\n",
      "{'predictions': np.int64(1)}\n",
      "{'predictions': np.int64(1)}\n",
      "{'predictions': np.int64(1)}\n",
      "{'predictions': np.int64(0)}\n",
      "{'predictions': np.int64(1)}\n",
      "{'predictions': np.int64(1)}\n",
      "{'predictions': np.int64(1)}\n",
      "{'predictions': np.int64(1)}\n",
      "{'predictions': np.int64(0)}\n",
      "{'predictions': np.int64(1)}\n",
      "{'predictions': np.int64(1)}\n",
      "{'predictions': np.int64(1)}\n",
      "{'predictions': np.int64(0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predict_xgboost(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
